{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12ecae26-07d0-472e-a7a8-7bf5258039a2",
   "metadata": {},
   "source": [
    "# Loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b133d808-cf7a-4b00-8435-5a5bf3f97563",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "plt.rcParams['text.usetex'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e6ebf1-c6f9-498c-abd2-b131170f4fb5",
   "metadata": {},
   "source": [
    "# Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8231438-4f8f-4687-a202-b3c9abd2b9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 37)\n",
      "Index(['Static Pressure', 'LPM Rota', 'temp_mcu', 'Static_Pa', 'SP_mV',\n",
      "       'DP_mV', 'Differential_Pa', 'Flow_lph', 'SB lpm', 'Accuracy', 'Error',\n",
      "       'ideal flow ', 'dcoef', 'LPM rota normalised', 'P pa (SB), 1',\n",
      "       'dP SB, 1 ', 'Flow SB lph 1'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataPath = \"/Users/duuta/inclusiveEnergy-/LabFit/240902-MPX10DP10mmdatacollection-MPX-1.csv\"\n",
    "data = pd.read_csv(dataPath)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# rename specific cols \n",
    "data.rename(columns={\"L PM Rota\":\"LPM_Rota\", 'cd':'dcoef'}, inplace=True)\n",
    "data0=data.drop(columns={'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'SB lpm.1', 'Accuracy.1', 'Error.1', 'Unnamed: 23', 'Unnamed: 24',\n",
    "       'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28',\n",
    "       'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32',\n",
    "       'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36'})\n",
    "print(data0.columns)\n",
    "\t\t\n",
    "# labFit CONSTANTs\n",
    "A=0.21334\n",
    "B=0.14933\n",
    "C=0.033616\n",
    "D=0.023546\n",
    "\n",
    "# standard constants\n",
    "output_lpm=100000  # atm pressure in Pa\n",
    "Tsb=30.66         # \n",
    "KCorrection=273.15          # Kelvin temp correction \n",
    "Tn=293.15  # some constant, not sure what exactly it is... some kind of temperature in degrees (20), then Kelvin corrected\n",
    "A1=0.0005366622509\n",
    "A2=0.0002010619298\n",
    "r0=1.184\n",
    "\n",
    "\n",
    "\n",
    "# epsilon\n",
    "epsilon=1e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17ab4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data0.shape)\n",
    "data0.duplicated().where(data0.duplicated() == True).count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9e8280",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (4072982662.py, line 212)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 212\u001b[0;36m\u001b[0m\n\u001b[0;31m    return { 'y_actual': y , 'lm_model':lm_values, 'dTree_model':dTree_values, 'poly_model':poly_values})\u001b[0m\n\u001b[0m                                                                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# class to get data, transform and export labfit data\n",
    "\n",
    "import csv \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.linear_models import  LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# look class with CSV module python\n",
    "# use pandas for duplications\n",
    "# might have to use pandas for the implementations\n",
    "\n",
    "# (data0.Differential_Pa - min(data0.Differential_Pa))/ (max(data0.Differential_Pa) - min(data0.Differential_Pa)) \n",
    "\n",
    "# not sure where I put the decorator\n",
    "# but can stll implement the different sections of the class object\n",
    "#\n",
    "\n",
    "# implement feature monitoring \n",
    "# implement data cleansing at source\n",
    "# need to udnder the where the data is stored\n",
    "# is the data stored? \n",
    "\n",
    "class DataSource(Enum):\n",
    "    def __init__(self):\n",
    "        raspberry_source = 1\n",
    "        other_soure = 2\n",
    "\n",
    "class DataRetriever:\n",
    "    def __init__(self, source= [Enum], is_connected):\n",
    "        self.source = source\n",
    "        self.is_connected = is_connected\n",
    "\n",
    "    def connector(self):\n",
    "        if not self.is_connected:\n",
    "            'establish connection'\n",
    "        return self.is_connected\n",
    "\n",
    "    def get_data(self):\n",
    "        if connector():\n",
    "            'source data and write to csv or EXcel'\n",
    "        return 0  \n",
    "\n",
    "class DataManager(self):\n",
    "    def __init__(self, filename,  usecols=['Static Pressure', 'LPM Rota', 'temp_mcu', 'Static_Pa', 'SP_mV',\n",
    "       'DP_mV', 'Differential_Pa', 'Flow_lph', 'ideal flow ', 'cd',], **kwargs,):\n",
    "        self.filename = filename\n",
    "        self.data = pd.read_csv(self.filename, usecols, **kwargs)\n",
    "\n",
    "    def get_col(self, col ):\n",
    "        'Implement one or many cols of data...'\n",
    "        if not all(item in self.data.columns.values for item in col):\n",
    "            raise KeyError(f\"{col} not in data columns\")\n",
    "        return self.data.loc[:, col] \n",
    "\n",
    "    def get_row(self, row):\n",
    "        'Implement for one or many row...'\n",
    "        if not all(item in self.data.columns.values for item in row):\n",
    "            raise KeyError(f\"{row} not in data rows\")\n",
    "        return self.data.loc[row,:]\n",
    "    \n",
    "    def get_nrows(self):\n",
    "        \"\"\"\n",
    "            returns the number of rows for data \n",
    "        \"\"\"\n",
    "        return self.data.count()\n",
    "\n",
    "    def get_ncols(self):\n",
    "        \"returns the number of cols for data\"\n",
    "        return len(self.data.columns)       \n",
    "\n",
    "    def Deduplicates(self):\n",
    "        \"return data without duplicates...\"\n",
    "        return self.data.drop_duplicates(inplace=)\n",
    "\n",
    "\n",
    "class DataTransformer:\n",
    "    def __init__(self, data_object,  **kwargs):\n",
    "        self.data_Object=data_object \n",
    "\n",
    "    \n",
    "    def transform_variable(self, scaling_type='min-max', epsilon, axis=1, **kwargs):\n",
    "        \"\"\"\n",
    "        return scaled versions of desired variables [typically scales to (0, 1)]\n",
    "        \"\"\"\n",
    "        for col in self.data_Object:\n",
    "            col_data = self.data_Object[col]\n",
    "\n",
    "            match scaling_type:\n",
    "                case 'min-max' | 'normalization':\n",
    "                    max_value = col_data.max()\n",
    "                    min_value = col_data.min()\n",
    "                    col_transformed = (col_data - min_value)/ (max_value - min_value)\n",
    "                \n",
    "                case 'standardized':\n",
    "                    mean = col_data.mean()\n",
    "                    std = col_data.std() + epsilon \n",
    "                    col_transformed = (col_data - mean)/std\n",
    "                \n",
    "                case _:\n",
    "                    max_value = col_data.max()\n",
    "                    min_value = col_data.min()\n",
    "                    col_transformed = (col_data - min_value)/ (max_value - min_value)\n",
    "\n",
    "            self.data_Object[col] = col_transformed  \n",
    "        return data_Object \n",
    "\n",
    "\n",
    "    def is_data_transformed(self, scaling_type, tolerance):\n",
    "        \"\"\"\n",
    "        Check if the dataset is appropriately scaled.\n",
    "\n",
    "        Parameters:\n",
    "        - df: The DataFrame containing the data\n",
    "        - scaling_type: 'standardized' or 'normalized' or 'min-max' to check the scaling type\n",
    "        - tolerance: A tolerance value for mean, std, min, max to allow small deviations\n",
    "    \n",
    "        Returns:\n",
    "        - bool, indicating whether the data is transformed \n",
    "        \"\"\"\n",
    "    \n",
    "        for column in self.data.columns:\n",
    "            column_data = self.data[column]\n",
    "            \n",
    "            match scaling_type:\n",
    "                case 'standardized':\n",
    "                    # Check mean and std for standardized data (mean ≈ 0, std ≈ 1)\n",
    "                    mean = column_data.mean()\n",
    "                    std_dev = column_data.std()\n",
    "                    is_scaled = (abs(mean) < tolerance) and (abs(std_dev - 1) < tolerance)\n",
    "                \n",
    "                    # need to handle what happens if is_scaled is false \n",
    "\n",
    "                case 'normalized':\n",
    "                    # Check min and max for normalized data (min ≈ 0, max ≈ 1)\n",
    "                    min_val = column_data.min()\n",
    "                    max_val = column_data.max()\n",
    "                    is_scaled = (abs(min_val) < tolerance) and (abs(max_val - 1) < tolerance)\n",
    "                \n",
    "                    # need to handle what happens if is_scaled is false \n",
    "            \n",
    "                case 'min-max':\n",
    "                    # Check min and max for min-max normalized data (min ≈ 0 , max ≈ 1 )\n",
    "                    min_val = column_data.min()\n",
    "                    max_val = column_data.max()\n",
    "                    is_scaled = (abs(min_val) < tolerance ) and (abs(max_val - 1) < tolerance )\n",
    "                    # need to handle what happens if is_scaled is false \n",
    "\n",
    "                case _:\n",
    "                    raise ValueError(\"Invalid scaling_type. Use 'standardized' or 'normalized'.\")\n",
    "    \n",
    "        return is_scaled\n",
    "\n",
    "\n",
    "        def convert_to_flow_lph(self, col):\n",
    "            if not col:\n",
    "                return self.data['Flow_lph']/60\n",
    "            else:\n",
    "                return self.data[col]/60 \n",
    "\n",
    "\n",
    "        def apply_lpm_rota_normalization(self, output_lpm, Tn, KCorrection):\n",
    "            data = self.data_Object\n",
    "            return data.LPM_Rota *(data.Static_Pa + output_lpm)/ output_lpm*(Tn/(data.temp_mcu + KCorrection))\n",
    "\n",
    "\n",
    "\n",
    "class DataAnalyzer:\n",
    "    def __init__(self, data_Object):\n",
    "        self.data_Object = data_Object\n",
    "    \n",
    "\n",
    "    def fit_data(self, X, y, **kwargs, random_state=42):\n",
    "        \"\"\"\"\n",
    "        return fitDict of multiple fits for data... including:\n",
    "        polynomial fit, decision trees, regresssion \n",
    "        \n",
    "        model_values: takes an order pair, model_score, model_mse, and predicted model values\n",
    "         but poly 2D which takes the coeffs, model_mse, and model_predicted \n",
    "        \"\"\"\n",
    "\n",
    "        # init, fit and predict linear regression  model\n",
    "        lm_model = LinearRegression(random_state=42)\n",
    "        \n",
    "        lm_score = lm_model.score(X, y)\n",
    "        lm_y  = lm_model.predict(X)\n",
    "        lm_mse = compute_residuals(y - lm_y)\n",
    "\n",
    "        lm_values = make_set_of_metrics(lm_score, lm_mse, lm_y)\n",
    "        \n",
    "        \n",
    "        # init, fit and predict decision tree\n",
    "        dTree_model = DecisionTreeRegressor(random_state=42)\n",
    "        \n",
    "        dTree_score = dTree_model.score(X, y)\n",
    "        dTree_mse = compute_residuals(y - dTree_y)\n",
    "        dTree_y  = dTree_model.predict(X)\n",
    "\n",
    "        dTree_values = make_set_of_metrics(dTree_score, dTree_mse, dTree_y)\n",
    "        \n",
    "        \n",
    "        # designX, fit and predict 2 order polynomial \n",
    "        X_design = design_X(X)\n",
    "        \n",
    "        coef, _, _, _= np.linalg.lstsq(X_design, y)\n",
    "        poly_y = X_design @ coef\n",
    "        poly_mse = compute_residuals(y - poly_y)\n",
    "\n",
    "        poly_values = make_set_of_metrics(coef, poly_mse, poly_y)\n",
    "        \n",
    "        # set dict values        \n",
    "        return { 'y_actual': y , 'lm_model':lm_values, 'dTree_model':dTree_values, 'poly_model':poly_values} \n",
    "\n",
    "\n",
    "\n",
    "class DataVisualizer:\n",
    "    def __init__(self, fitDict, **kwargs,  ):\n",
    "        self.fitDict = fitDict\n",
    "\n",
    "    def apply_model_visualizer(self, color='blue', figsize=(20, 20), all, **kwargs):\n",
    "\n",
    "            \"\"\"\n",
    "            outputs a plot of all model fits for data, with the default all flag. Working on\n",
    "            additional flags to allow for flexibilty in comparison. \n",
    "            \"\"\"\n",
    "            fitDict = self.fitDict\n",
    "\n",
    "            p=plt.rcParams\n",
    "            p[\"figure.figsize\"] = figsize\n",
    "            p[\"font.sans-serif\"] = [\"Roboto Condensed\"]\n",
    "            p[\"font.weight\"] = \"light\"\n",
    "            p[\"ytick.minor.visible\"] = \"True\"\n",
    "            p[\"xtick.minor.visible\"] = \"True\"\n",
    "\n",
    "            # define data (fix outputs for model_mse)\n",
    "            y_actual = fitDict['y_actual']\n",
    "            lm_score, y_lm =  fitDict['lm_model']\n",
    "            poly_mse, y_poly =fitDict['poly_model'][1] \n",
    "            dTree_score, y_dTree = fitDict['dTree_model'][1]\n",
    "            \n",
    "\n",
    "            fig0 = plt.figure(constrainted_layout=True)\n",
    "            nrows, ncols= 1, 3\n",
    "            w1, w2 = 20, 1\n",
    "            gspec=gridspec.GridSpec(ncols=ncols, nrows=nrows, figure=fig0, width_ratios=[w1, w2]))\n",
    "\n",
    "            # \n",
    "            ax = plt.subplot(gspec[0, 0], aspect=1)\n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.set_xticks(np.linspace(0, 1, 4 + 1))\n",
    "            ax.set_xlabel(\"X instances in time\")\n",
    "            ax.set_ylim(0, 1)\n",
    "            ax.set_yticks(np.linspace(0, 1, 4 + 1))\n",
    "            ax.set_ylabel(\"Discharge Coefficient\", fontsize=20)\n",
    "            ax.set_title(\"Title\", family=\"Roboto\", weight=500)\n",
    "            ax.axline((0.00, 0.00), slope=1.0, linestyle='--', color='red', linewidth=2.0)\n",
    "            ax.set_title(f\"actual model: y-actual vs y_pred ({ lm_score=:.3f}) {lm_mse=:.3f}\", fontsize=20)\n",
    "            I = ax.scatter(y_actual, y_lm, color=color)\n",
    "\n",
    "            # Decision tree compare\n",
    "            ax = plt.subplot(gspec[0, 1], aspect=1)\n",
    "            ax.set_xlabel(\"X instances in time\", fontsize=20)\n",
    "            ax.set_title(f\"DecisionTreeRegressor: y-actual vs y_pred ({dTree_score=:.3f} {dTree_mse=:.3f})\", fontsize=20)\n",
    "            ax.axline((0.0, 0.0), slope=1, linestyle='--', color='red', linewidth=2.0)\n",
    "            ax.scatter(y_actual, y_dTree, color=color)\n",
    "\n",
    "            # poly plot \n",
    "            ax = plt.subplot(gspec[0, 2], aspect=1)\n",
    "            ax.set_xlabel(\"X instances in time\", fontsize=20)\n",
    "            ax.axline((0.0, 0.0), slope=1, linestyle='--', color='red', linewidth=2.0)\n",
    "            ax.set_title(f\"$f(x, y) = a x^2 + b xy + cx+ dy+ ey^2 + f$: y-actual vs y_pred (R_sq={None} {poly_mse=:.3f})\", fontsize=20)\n",
    "            I = ax.scatter(y_actual, y_poly, color=color)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e4c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Obj =inclusiveEnergyDataObject(filename=dataPath, Tn=Tn, output_lpm=output_lpm, kCorrection=KCorrection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9ab641",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_Obj.remove_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f3a33-8dd6-40ea-89c8-9ea3852f7dfe",
   "metadata": {},
   "source": [
    "# Meta-functions for computations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3307874-3fe5-4472-9b41-2cc1bdad4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB choice of best model is not based on accuracy is also based on functional form and possible runins resulting from \n",
    "# bad functional form \n",
    "# X1 : 'Static_Pa'  X2: 'Differential_Pa'\n",
    "#data['labfit'] = data.apply(labfit_model, axis=1)\n",
    "#data['labfit0'] = data.apply(lambda row: labfit_function(row), axis=1)\n",
    "# labfit_best1 : Y=A/X2+B*EXP(C*X1)+D/X2**2 \n",
    "# labfit_best0 : Y=A*X1**(B*X2**C)+D/X2\n",
    "\n",
    "# Reduced Chi_squared \n",
    "R00 = 0.136442E-01\n",
    "R01 = 0.967780E-02\n",
    "\n",
    "labfit1 = lambda row: A*row.Differential_Pa + B*np.exp(C*row.Static_Pa) + D/row.Differential_Pa**2 \n",
    "labfit0 = lambda row: A*row.Differential_Pa**(B*row.Static_Pa**C) + D/row.Static_Pa\n",
    "\n",
    "\n",
    "labfit1_scaled = lambda row: A*row.DP_scale + B*np.exp(C*row.SP_scale) + D/(row.DP_scale**2 + epsilon)\n",
    "labfit0_scaled = lambda row: A*row.DP_scale**(B*row.SP_scale**C) + D/(row.SP_scale + epsilon)\n",
    "\n",
    "\n",
    "LPM_rota_normalization = lambda row: row.LPM_Rota *(row.Static_Pa + output_lpm)/ output_lpm*(Tn/(row.temp_mcu + c))\n",
    "SB_lpm = lambda row: row.Flow_lph/60 \n",
    "\n",
    "\n",
    "def flow_amounts(dcoeff, Differential_Pa):\n",
    "    return dcoeff*A2*np.sqrt(2*Differential_Pa)/(r0*(A2/A1)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef61544-8265-44d3-8f7c-4a0b9e6c6167",
   "metadata": {},
   "source": [
    "# Computing desired variables from system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
